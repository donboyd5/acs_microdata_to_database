---
editor_options: 
  chunk_output_type: console
editor:
  markdown:
    canonical: true
execute:
  output: true    
---

# Convert ACS microdata and documentation to database tables

## Setup

```{r}
#| label: setup

tprint <- 75 # default tibble print
options(tibble.print_max = tprint, tibble.print_min = tprint)

library(rlang)
library(here)
library(rprojroot)
library(this.path)
# library(lintr)
# library(styler)
library(btools)

library(tidyverse)
library(dplyr) # dplyr functions included with tidyverse, only if specifically loaded elsewhere
# remotes::install_github("JanMarvin/openxlsx2")
library(openxlsx2) # for writing xlsx files
library(readxl)
library(vroom)
library(fs)
library(skimr)
library(Hmisc)
library(lubridate)
library(formattable)
library(janitor)
library(vtable)
library(quarto)

# https://duckplyr.tidyverse.org/
library(dbplyr)
library(DBI)
library(duckplyr)

```

```{r}
#| label: locations

DACS5 <- r"(D:\data\acs\5year)"
DACS52023 <- fs::path(DACS5, "2023")
ACSDB <- "acs5year.duckdb"
path_duckdb <- fs::path(DACS5, ACSDB)

```

## 2023 data

### Household records

```{r}
#| label: ONETIME-read-clean-hus
#| eval: true

zfn <- "csv_hus.zip" # has a, b, c, d e.g., c is psam_pusc.csv
zpath <- fs::path(DACS52023, zfn)
zpath

(fnames <- utils::unzip(zpath, list = TRUE)$Name |> str_subset(".csv"))

# make as many columns as possible integer - selectively making others character or numeric
# REMEMBER that long code lists (eg., occ codes) could have a character in them!!
char_vars <- c("RT", "SERIALNO")
char_list <- setNames(rep(list(col_character()), length(char_vars)), char_vars)
# only create numeric (as opposed to integer) variables that might exceed integer limit of ~ 2.1 billion
num_vars <- c("ADJHSG", "ADJINC", "FINCP", "GRNTP", "HINCP")
# make weight variables WGTP numeric by conversion, later
num_list <- setNames(rep(list(col_number()), length(num_vars)), num_vars)

colspec <- do.call(cols, c(char_list, num_list, list(.default = col_integer())))
colspec

connections <- purrr::map(fnames, \(x) unz(zpath, x))
# hrecs <- vroom(connections, id = "fname", n_max=5, col_types = colspec)
a <- proc.time()
hrecs <- vroom(connections, id = "fname", col_types = colspec) # 30 secs for my defs
b <- proc.time()
b - a
# glimpse(hrecs)
# summary(hrecs)

hrecs2 <- hrecs |>
  rename_with(str_to_lower) |>
  mutate(fname = str_replace(fname, ".*:", "")) |>
  mutate(adjhsg = adjhsg / 1e6, adjinc = adjinc / 1e6)
ns(hrecs2)
count(hrecs2, fname)
# system.time(summary(hrecs2)) # ~ 32 secs
# system.time(missing_counts <- colSums(is.na(hrecs2))) # 10 secs
# missing_counts

count(hrecs2, accessinet)
count(hrecs2, cplt)

# end reading and cleaning data ---

```

```{r}
#| label: ONETIME-save-households-to-database
#| eval: true

# note that we save to the master database for all ACS 5-year-file years
acsdb <- DBI::dbConnect(duckdb::duckdb(path_duckdb))
DBI::dbListTables(acsdb)
system.time(DBI::dbWriteTable(acsdb, "hus_5year2023", hrecs2, overwrite = TRUE)) # 35 secs
DBI::dbListTables(acsdb)
DBI::dbDisconnect(acsdb)

```

### Person records

```{r}
#| label: ONETIME-read-clean-pus
#| eval: true

zfn <- "csv_pus.zip" # has a, b, c, d e.g., c is psam_pusc.csv
zpath <- fs::path(DACS52023, zfn)
zpath

(fnames <- unzip(zpath, list = TRUE)$Name |> str_subset(".csv"))

# make as many columns as possible integer - selectively making others character or numeric
# REMEMBER that long code lists (eg., occ codes) could have a character in them!!
char_vars <- c("RT", "SERIALNO", "NAICSP", "SOCP")
char_list <- setNames(rep(list(col_character()), length(char_vars)), char_vars)
# only create numeric (as opposed to integer) variables that might exceed integer limit of ~ 2.1 billion
num_vars <- c("ADJINC", "PINCP")
# make weight variables WGTP numeric by conversion, later
num_list <- setNames(rep(list(col_number()), length(num_vars)), num_vars)

colspec <- do.call(cols, c(char_list, num_list, list(.default = col_integer())))
colspec


connections <- purrr::map(fnames, \(x) unz(zpath, x))
# hrecs <- vroom(connections, id = "fname", n_max=5, col_types = colspec)
a <- proc.time()
precs <- vroom(connections, id = "fname", col_types = colspec) # 30 secs for my defs
b <- proc.time()
b - a # ~ 75 secs
# glimpse(precs)
# summary(precs)

precs2 <- precs |>
  rename_with(str_to_lower) |>
  mutate(fname = str_replace(fname, ".*:", ""), adjinc = adjinc / 1e6)

count(precs2, fname)

# end read and clean person records ----

```

```{r}
#| label: ONETIME-save-persons-to-database
#| eval: true

# note that we save to the master database for all ACS 5-year-file years
acsdb <- DBI::dbConnect(duckdb::duckdb(path_duckdb))
DBI::dbListTables(acsdb)
system.time(DBI::dbWriteTable(acsdb, "pus_5year2023", precs2, overwrite = TRUE))
DBI::dbListTables(acsdb)
DBI::dbDisconnect(acsdb)

```


## 2023 data codes

## Data code list


## 2020 PUMA codes -- for ACS 5-year 2019-2023 but ALSO for some other years

Oddly there doesn't seem to be a good computerized version of 2020 PUMA codes and names. I found an [html table on the Census tigerweb site](https://tigerweb.geo.census.gov/tigerwebmain/Files/acs25/tigerweb_acs25_puma_2020_tab20_us.html). I copied it into an Excel file and put it in the "derived_documentation" folder of this project so that it will be included in pushes to github.

This section reads the file and puts it into the ACS database.

```{r}
#| label: get-puma-codes

DDOC <- here::here("derived_documentation")
fname <- "puma2020_codelist.xlsx"
fpath <- fs::path(DDOC, fname)

pumacodes1 <- read_excel(
  fpath,
  sheet = "puma2020",
  skip = 1,
  col_types = "text"
)
count(pumacodes1, STATE)
# can't convert these next 2 to numeric as they have character values
count(pumacodes1, LSADC)
count(pumacodes1, FUNCSTAT)

pumacodes2 <- pumacodes1 |>
  rename_with(tolower) |>
  mutate(
    across(c(state, pumace), as.integer),
    across(
      c(
        pop100,
        hu100,
        arealand,
        areawater,
        centlat,
        centlon,
        intptlat,
        intptlon
      ),
      as.numeric
    )
  )
summary(pumacodes2)
skimr::skim_without_charts(pumacodes2) # make sure NO missing values from the numeric conversion

# rename as needed
pumacodes3 <- pumacodes2 |>
  rename(pumaname = name, puma = pumace)

nycheck <- pumacodes3 |> filter(state == 36) # 144 rows

```

```{r}
#| label: ONETIME-save-pumacodes-to-database
#| eval: true

# note that we save to the master database for all ACS 5-year-file years
acsdb <- DBI::dbConnect(duckdb::duckdb(path_duckdb))
DBI::dbListTables(acsdb)
DBI::dbWriteTable(
  acsdb,
  "pumacodes_2020",
  pumacodes3,
  overwrite = TRUE
)
DBI::dbListTables(acsdb)
DBI::dbDisconnect(acsdb)

```


## Get socp occupation codes

```{r}
#| label: prepare-occ-codes

fname <- "ACSPUMS2019_2023CodeLists.xlsx"
xpath <- fs::path(DACS52023, "documentation", fname)
occsheet <- "OCCP & SOCP"

occ1 <- read_excel(
  xpath,
  sheet = occsheet,
  range = cellranger::cell_rows(c(11, NA)),
  col_names = c("pumsocc", "sococc", "description")
)

occ2 <- occ1 |>
  filter(!is.na(pumsocc)) |>
  filter(str_detect(pumsocc, "-", negate = TRUE)) |>
  filter(str_detect(pumsocc, "Note|Highlight", negate = TRUE)) |>
  arrange(pumsocc, sococc)

# are any codes are not unique?
anyDuplicated(occ2$pumsocc)
anyDuplicated(occ2$sococc)
# no, good

occ3 <- occ2 |>
  mutate(socp = str_remove(sococc, "-")) |>
  relocate(socp, .after = sococc)

```

```{r}
#| label: ONETIME-save-occupation-codes-to-database
#| eval: true

# note that we save to the master database for all ACS 5-year-file years
acsdb <- DBI::dbConnect(duckdb::duckdb(path_duckdb))
DBI::dbListTables(acsdb)
DBI::dbWriteTable(acsdb, "occcodes_5year2023", occ3, overwrite = TRUE)
DBI::dbListTables(acsdb)
DBI::dbDisconnect(acsdb)

```

```{r}
#| label: check-occ-codes

acsdb <- DBI::dbConnect(duckdb::duckdb(path_duckdb))
DBI::dbListTables(acsdb)

pocc <- tbl(acsdb, "pus_5year2023") |>
  filter(!is.na(socp)) |>
  select(socp) |>
  dplyr::distinct() |>
  mutate(src = "person") |>
  arrange(socp) |>
  as_tibble() # person occ codes

socc <- tbl(acsdb, "occcodes_5year2023") |>
  as_tibble()

# how well do they merge?

check <- socc |>
  left_join(pocc, by = join_by(socp))

check |>
  filter(is.na(src))
# good, all match
```  


## Check the database
  
```{r}
#| label: checks

acsdb <- DBI::dbConnect(duckdb::duckdb(path_duckdb))
DBI::dbListTables(acsdb)
DBI::dbDisconnect(acsdb)

```

## Explore

```{r}
#| eval: false

acsdb <- DBI::dbConnect(duckdb::duckdb(path_duckdb))
DBI::dbListTables(acsdb)

df <- tbl(acsdb, "pus_5year2023")
nyacs <- df |>
  dplyr::filter(state == "36")

soccodes <- count(nyacs, socp, sort = TRUE) |>
  as_tibble()

nurses <- nyacs |>
  dplyr::filter(str_sub(socp, 1, 2) == "29") |>
  as_tibble()
DBI::dbDisconnect(acsdb)

ny <- tbl(acsdb, "hus_5year2023") |>
  dplyr::filter(state == 36) |>
  select(1:20) |>
  as_tibble()
glimpse(ny)

```

## Old code

```{r}
#| label: code-info
#| eval: false

# acsdb <- DBI::dbConnect(RSQLite::SQLite(), fs::path(DACS, "acs.sqlite"))
# DBI::dbWriteTable(acsdb, "pus", df2)
# DBI::dbListTables(acsdb)
# # DBI::dbExecute(acsdb, "ALTER TABLE pus RENAME TO pus_5year2023;")
# DBI::dbDisconnect(acsdb)

# con <- DBI::dbConnect(duckdb::duckdb(path_duckdb))
# DBI::dbListTables(con)
# DBI::dbExecute(con, "ALTER TABLE data RENAME TO pus_5year2023;")
# DBI::dbDisconnect(con)

# renamed original files from Martin from td15.zip to td15_pw.zip (for password, etc.)
# gzpre <- "td"
# gzsfx <- ".zip"
# gzfiles <- paste0(gzpre, c(15, 21, 23), gzsfx)
# (gzpaths <- path(dtdfiles, gzfiles))

# use this when we have year on the data files, but not until then
# tctd <- open_dataset(
#   # sources = fs::path(dyfiles, "tax_microdata_2015.csv.gz"),
#   sources = gzpaths,
#   col_types = schema(ISBN = string()),
#   format = "csv"
# )
# count(tctd, FLPDYR) |> collect()

# df1 <- vroom(gzpaths, id = "fname")

```